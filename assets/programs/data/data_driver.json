{
  "id": "4",
  "name": "Data Driver",
  "shortDescription": "Transform data into actionable insights and business value",
  "fullDescription": "Master the art and science of data analytics and data science. Learn to collect, process, analyze, and visualize data to drive business decisions using modern tools and techniques.",
  "duration": "11 weeks",
  "difficulty": "beginner",
  "thumbnailPath": "assets/programs/data.webp",
  "author": "Maria Santos",
  "authorBio": "Data scientist with a background in statistics and 8 years of experience in business intelligence. Published researcher and former analytics director at startups.",
  "isComingSoon": false,
  "modules": [
    {
      "title": "Data Analytics Foundations",
      "description": "Statistics, probability, and analytical thinking",
      "duration": "2 weeks",
      "order": 1,
      "content": "# Data Analytics Foundations\n\n## Welcome to Data Analytics\n\nData analytics is the science of examining raw data to draw conclusions and make informed decisions. In this module, you'll build a strong foundation in statistical thinking and analytical methodologies.\n\n## What is Data Analytics?\n\nData analytics involves:\n- **Collecting** relevant data from various sources\n- **Cleaning** and preparing data for analysis\n- **Analyzing** data using statistical methods\n- **Visualizing** insights for stakeholders\n- **Communicating** findings to drive decisions\n\n## The Data Analytics Process\n\n### 1. Define the Question\n\nEvery analysis starts with a clear question:\n- What problem are we trying to solve?\n- What decisions need to be made?\n- What metrics matter?\n\n**Example Questions**:\n- Why are sales declining in Region X?\n- Which customer segment is most profitable?\n- How can we reduce customer churn?\n\n### 2. Collect Data\n\nData sources include:\n- **Internal**: Databases, CRM systems, logs\n- **External**: APIs, public datasets, web scraping\n- **Primary**: Surveys, experiments\n- **Secondary**: Published reports, studies\n\n### 3. Clean Data\n\nData cleaning is crucial:\n- Handle missing values\n- Remove duplicates\n- Fix inconsistencies\n- Standardize formats\n- Validate accuracy\n\n### 4. Analyze Data\n\nApply appropriate techniques:\n- Descriptive statistics\n- Exploratory data analysis\n- Statistical testing\n- Predictive modeling\n\n### 5. Visualize & Communicate\n\nPresent findings effectively:\n- Charts and graphs\n- Dashboards\n- Reports\n- Presentations\n\n## Descriptive Statistics\n\n### Measures of Central Tendency\n\n**Mean (Average)**\n```python\nimport numpy as np\n\ndata = [23, 45, 67, 34, 89, 12, 56, 78, 90, 34]\nmean = np.mean(data)\nprint(f\"Mean: {mean}\")\n# Output: Mean: 52.8\n```\n\n**Median (Middle Value)**\n```python\nmedian = np.median(data)\nprint(f\"Median: {median}\")\n# Output: Median: 50.5\n```\n\n**Mode (Most Frequent)**\n```python\nfrom scipy import stats\n\nmode_result = stats.mode(data, keepdims=True)\nprint(f\"Mode: {mode_result.mode[0]}\")\n# Output: Mode: 34\n```\n\n### Measures of Spread\n\n**Range**\n```python\ndata_range = np.max(data) - np.min(data)\nprint(f\"Range: {data_range}\")\n# Output: Range: 78\n```\n\n**Variance**\n```python\nvariance = np.var(data)\nprint(f\"Variance: {variance:.2f}\")\n```\n\n**Standard Deviation**\n```python\nstd_dev = np.std(data)\nprint(f\"Standard Deviation: {std_dev:.2f}\")\n```\n\n**Quartiles and IQR**\n```python\nq1 = np.percentile(data, 25)\nq2 = np.percentile(data, 50)  # Same as median\nq3 = np.percentile(data, 75)\niqr = q3 - q1\n\nprint(f\"Q1: {q1}, Q2: {q2}, Q3: {q3}\")\nprint(f\"IQR: {iqr}\")\n```\n\n## Probability Basics\n\n### Basic Probability Rules\n\n**Probability of an Event**\n```\nP(A) = Number of favorable outcomes / Total number of outcomes\n```\n\n**Complement Rule**\n```\nP(not A) = 1 - P(A)\n```\n\n**Addition Rule**\n```\nP(A or B) = P(A) + P(B) - P(A and B)\n```\n\n**Multiplication Rule (Independent Events)**\n```\nP(A and B) = P(A) × P(B)\n```\n\n### Probability Distributions\n\n**Normal Distribution**\n```python\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Generate normal distribution\nmu, sigma = 0, 1\nx = np.linspace(-4, 4, 100)\ny = norm.pdf(x, mu, sigma)\n\nplt.plot(x, y)\nplt.title('Normal Distribution (μ=0, σ=1)')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.grid(True)\nplt.show()\n```\n\n**Binomial Distribution**\n```python\nfrom scipy.stats import binom\n\n# 10 coin flips, probability of heads = 0.5\nn, p = 10, 0.5\nx = np.arange(0, 11)\ny = binom.pmf(x, n, p)\n\nplt.bar(x, y)\nplt.title('Binomial Distribution (n=10, p=0.5)')\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.show()\n```\n\n## Exploratory Data Analysis (EDA)\n\n### Loading and Inspecting Data\n\n```python\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('sales_data.csv')\n\n# First look at the data\nprint(df.head())\nprint(df.info())\nprint(df.describe())\n\n# Check for missing values\nprint(df.isnull().sum())\n\n# Data types\nprint(df.dtypes)\n```\n\n### Univariate Analysis\n\n```python\nimport seaborn as sns\n\n# Distribution of a numerical variable\nplt.figure(figsize=(10, 6))\nsns.histplot(df['sales'], kde=True, bins=30)\nplt.title('Sales Distribution')\nplt.xlabel('Sales Amount')\nplt.ylabel('Frequency')\nplt.show()\n\n# Box plot to identify outliers\nplt.figure(figsize=(8, 6))\nsns.boxplot(y=df['sales'])\nplt.title('Sales Box Plot')\nplt.show()\n\n# Categorical variable analysis\ndf['category'].value_counts().plot(kind='bar')\nplt.title('Product Category Distribution')\nplt.xlabel('Category')\nplt.ylabel('Count')\nplt.show()\n```\n\n### Bivariate Analysis\n\n```python\n# Scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(df['marketing_spend'], df['sales'], alpha=0.5)\nplt.title('Marketing Spend vs Sales')\nplt.xlabel('Marketing Spend ($)')\nplt.ylabel('Sales ($)')\nplt.show()\n\n# Correlation matrix\ncorr_matrix = df.corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Matrix')\nplt.show()\n\n# Group analysis\ndf.groupby('region')['sales'].mean().plot(kind='bar')\nplt.title('Average Sales by Region')\nplt.xlabel('Region')\nplt.ylabel('Average Sales')\nplt.show()\n```\n\n### Multivariate Analysis\n\n```python\n# Pair plot\nsns.pairplot(df[['sales', 'marketing_spend', 'price', 'quantity']])\nplt.show()\n\n# Grouped analysis\nsns.catplot(x='region', y='sales', hue='category', \n            kind='bar', data=df, height=6, aspect=1.5)\nplt.title('Sales by Region and Category')\nplt.show()\n```\n\n## Statistical Hypothesis Testing\n\n### The Hypothesis Testing Process\n\n1. **State the hypotheses**\n   - H₀ (Null Hypothesis): No effect/difference\n   - H₁ (Alternative Hypothesis): There is an effect/difference\n\n2. **Choose significance level (α)**: Usually 0.05 (5%)\n\n3. **Calculate test statistic**\n\n4. **Determine p-value**\n\n5. **Make decision**:\n   - If p-value < α: Reject H₀\n   - If p-value ≥ α: Fail to reject H₀\n\n### T-Test\n\n**One-Sample T-Test**\n```python\nfrom scipy.stats import ttest_1samp\n\n# Test if mean sales = $5000\nsample_data = df['sales']\nt_statistic, p_value = ttest_1samp(sample_data, 5000)\n\nprint(f\"T-statistic: {t_statistic:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\nif p_value < 0.05:\n    print(\"Reject H0: Mean sales is significantly different from $5000\")\nelse:\n    print(\"Fail to reject H0: No significant difference\")\n```\n\n**Two-Sample T-Test**\n```python\nfrom scipy.stats import ttest_ind\n\n# Compare sales between two regions\nregion_a = df[df['region'] == 'North']['sales']\nregion_b = df[df['region'] == 'South']['sales']\n\nt_stat, p_value = ttest_ind(region_a, region_b)\n\nprint(f\"T-statistic: {t_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n```\n\n### Chi-Square Test\n\n```python\nfrom scipy.stats import chi2_contingency\n\n# Test relationship between category and region\ncontingency_table = pd.crosstab(df['category'], df['region'])\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\n\nprint(f\"Chi-square statistic: {chi2:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\nprint(f\"Degrees of freedom: {dof}\")\n```\n\n### ANOVA (Analysis of Variance)\n\n```python\nfrom scipy.stats import f_oneway\n\n# Compare sales across multiple regions\nregions = df['region'].unique()\nregion_groups = [df[df['region'] == r]['sales'] for r in regions]\n\nf_stat, p_value = f_oneway(*region_groups)\n\nprint(f\"F-statistic: {f_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n```\n\n## Correlation vs Causation\n\n**Important**: Correlation does NOT imply causation!\n\n```python\n# Calculate correlation\ncorrelation = df['marketing_spend'].corr(df['sales'])\nprint(f\"Correlation: {correlation:.4f}\")\n\n# Visualize\nplt.scatter(df['marketing_spend'], df['sales'])\nplt.title(f'Correlation: {correlation:.4f}')\nplt.xlabel('Marketing Spend')\nplt.ylabel('Sales')\nplt.show()\n```\n\n**Correlation Strength**:\n- 0.0 - 0.3: Weak\n- 0.3 - 0.7: Moderate\n- 0.7 - 1.0: Strong\n\n## Data Quality Issues\n\n### Handling Missing Data\n\n```python\n# Identify missing data\nprint(df.isnull().sum())\n\n# Drop rows with missing values\ndf_clean = df.dropna()\n\n# Fill with mean/median/mode\ndf['sales'].fillna(df['sales'].mean(), inplace=True)\ndf['category'].fillna(df['category'].mode()[0], inplace=True)\n\n# Forward/backward fill\ndf['value'].fillna(method='ffill', inplace=True)\n```\n\n### Handling Outliers\n\n```python\n# IQR method\nQ1 = df['sales'].quantile(0.25)\nQ3 = df['sales'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Remove outliers\ndf_no_outliers = df[\n    (df['sales'] >= lower_bound) & \n    (df['sales'] <= upper_bound)\n]\n\nprint(f\"Original size: {len(df)}\")\nprint(f\"After removing outliers: {len(df_no_outliers)}\")\n```\n\n## Practical Exercise\n\n**Task**: Analyze a retail dataset\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv('retail_sales.csv')\n\n# 1. Explore the data\nprint(\"Dataset Info:\")\nprint(df.info())\nprint(\"\\nSummary Statistics:\")\nprint(df.describe())\n\n# 2. Check data quality\nprint(\"\\nMissing Values:\")\nprint(df.isnull().sum())\n\n# 3. Visualize sales distribution\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nsns.histplot(df['sales'], kde=True)\nplt.title('Sales Distribution')\n\nplt.subplot(1, 3, 2)\nsns.boxplot(y=df['sales'])\nplt.title('Sales Box Plot')\n\nplt.subplot(1, 3, 3)\ndf['category'].value_counts().plot(kind='pie', autopct='%1.1f%%')\nplt.title('Category Distribution')\n\nplt.tight_layout()\nplt.show()\n\n# 4. Correlation analysis\ncorr = df[['sales', 'price', 'quantity', 'discount']].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n# 5. Hypothesis test: Compare sales across categories\nfrom scipy.stats import f_oneway\n\ncategories = df['category'].unique()\ngroups = [df[df['category'] == cat]['sales'] for cat in categories]\nf_stat, p_value = f_oneway(*groups)\n\nprint(f\"\\nANOVA Results:\")\nprint(f\"F-statistic: {f_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\nif p_value < 0.05:\n    print(\"Conclusion: Sales differ significantly across categories\")\nelse:\n    print(\"Conclusion: No significant difference in sales across categories\")\n```\n\n## Key Takeaways\n\n✅ **Always start with a clear question**  \n✅ **Understand your data before analyzing**  \n✅ **Clean data is crucial for accurate analysis**  \n✅ **Visualize data to uncover patterns**  \n✅ **Use appropriate statistical tests**  \n✅ **Correlation ≠ Causation**  \n✅ **Communicate findings clearly**  \n✅ **Document your analysis process**  \n\n## Resources\n\n- Khan Academy Statistics: https://www.khanacademy.org/math/statistics-probability\n- Scipy Stats Documentation: https://docs.scipy.org/doc/scipy/reference/stats.html\n- Seaborn Gallery: https://seaborn.pydata.org/examples/index.html\n- StatQuest YouTube: https://www.youtube.com/c/joshstarmer\n\n---\n\n**Next Module**: SQL & Database Management - where you'll learn to extract and manipulate data from databases!"
    },
    {
      "title": "SQL & Database Management",
      "description": "Query optimization and database design",
      "duration": "2 weeks",
      "order": 2,
      "content": "# SQL & Database Management\n\n## Introduction to SQL\n\nSQL (Structured Query Language) is the standard language for interacting with relational databases. It's essential for data analysts to extract, manipulate, and analyze data stored in databases.\n\n## Database Fundamentals\n\n### What is a Relational Database?\n\nA relational database stores data in tables with:\n- **Rows**: Individual records\n- **Columns**: Attributes/fields\n- **Primary Key**: Unique identifier for each row\n- **Foreign Key**: Links to another table\n\n### Database Design Principles\n\n**Normalization**: Organizing data to reduce redundancy\n- **1NF**: Each cell contains a single value\n- **2NF**: All non-key columns depend on the entire primary key\n- **3NF**: No transitive dependencies\n\n## SQL Basics\n\n### SELECT Statement\n\n```sql\n-- Select all columns\nSELECT * FROM customers;\n\n-- Select specific columns\nSELECT first_name, last_name, email \nFROM customers;\n\n-- Select with alias\nSELECT \n    first_name AS \"First Name\",\n    last_name AS \"Last Name\",\n    email AS \"Email Address\"\nFROM customers;\n```\n\n### WHERE Clause (Filtering)\n\n```sql\n-- Equality\nSELECT * FROM orders \nWHERE status = 'completed';\n\n-- Comparison operators\nSELECT * FROM products \nWHERE price > 100;\n\n-- Multiple conditions (AND)\nSELECT * FROM orders\nWHERE status = 'completed' \n  AND total_amount > 500;\n\n-- Multiple conditions (OR)\nSELECT * FROM products\nWHERE category = 'Electronics' \n   OR category = 'Computers';\n\n-- IN operator\nSELECT * FROM products\nWHERE category IN ('Electronics', 'Computers', 'Mobile');\n\n-- BETWEEN\nSELECT * FROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-12-31';\n\n-- LIKE (pattern matching)\nSELECT * FROM customers\nWHERE email LIKE '%@gmail.com';\n\n-- IS NULL\nSELECT * FROM customers\nWHERE phone_number IS NULL;\n```\n\n### ORDER BY (Sorting)\n\n```sql\n-- Ascending order (default)\nSELECT * FROM products\nORDER BY price;\n\n-- Descending order\nSELECT * FROM products\nORDER BY price DESC;\n\n-- Multiple columns\nSELECT * FROM products\nORDER BY category ASC, price DESC;\n```\n\n### LIMIT (Restricting Results)\n\n```sql\n-- Top 10 most expensive products\nSELECT * FROM products\nORDER BY price DESC\nLIMIT 10;\n\n-- Pagination (OFFSET)\nSELECT * FROM products\nORDER BY product_id\nLIMIT 20 OFFSET 40;  -- Skip first 40, get next 20\n```\n\n## Aggregate Functions\n\n```sql\n-- COUNT\nSELECT COUNT(*) AS total_customers\nFROM customers;\n\n-- COUNT DISTINCT\nSELECT COUNT(DISTINCT country) AS unique_countries\nFROM customers;\n\n-- SUM\nSELECT SUM(total_amount) AS total_revenue\nFROM orders\nWHERE status = 'completed';\n\n-- AVG\nSELECT AVG(price) AS average_price\nFROM products;\n\n-- MIN and MAX\nSELECT \n    MIN(price) AS cheapest,\n    MAX(price) AS most_expensive\nFROM products;\n```\n\n## GROUP BY\n\n```sql\n-- Sales by category\nSELECT \n    category,\n    COUNT(*) AS product_count,\n    AVG(price) AS avg_price,\n    SUM(stock_quantity) AS total_stock\nFROM products\nGROUP BY category;\n\n-- Orders by status\nSELECT \n    status,\n    COUNT(*) AS order_count,\n    SUM(total_amount) AS total_revenue\nFROM orders\nGROUP BY status;\n\n-- Monthly sales\nSELECT \n    DATE_TRUNC('month', order_date) AS month,\n    COUNT(*) AS orders,\n    SUM(total_amount) AS revenue\nFROM orders\nGROUP BY DATE_TRUNC('month', order_date)\nORDER BY month;\n```\n\n### HAVING (Filtering Groups)\n\n```sql\n-- Categories with more than 10 products\nSELECT \n    category,\n    COUNT(*) AS product_count\nFROM products\nGROUP BY category\nHAVING COUNT(*) > 10;\n\n-- High-value customer segments\nSELECT \n    customer_id,\n    COUNT(*) AS order_count,\n    SUM(total_amount) AS total_spent\nFROM orders\nGROUP BY customer_id\nHAVING SUM(total_amount) > 1000\nORDER BY total_spent DESC;\n```\n\n## JOINs\n\n### INNER JOIN\n\n```sql\n-- Orders with customer information\nSELECT \n    o.order_id,\n    o.order_date,\n    o.total_amount,\n    c.first_name,\n    c.last_name,\n    c.email\nFROM orders o\nINNER JOIN customers c ON o.customer_id = c.customer_id;\n```\n\n### LEFT JOIN\n\n```sql\n-- All customers and their orders (including customers with no orders)\nSELECT \n    c.customer_id,\n    c.first_name,\n    c.last_name,\n    COUNT(o.order_id) AS order_count,\n    COALESCE(SUM(o.total_amount), 0) AS total_spent\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id\nGROUP BY c.customer_id, c.first_name, c.last_name;\n```\n\n### Multiple JOINs\n\n```sql\n-- Order details with product information\nSELECT \n    o.order_id,\n    o.order_date,\n    c.first_name || ' ' || c.last_name AS customer_name,\n    p.product_name,\n    oi.quantity,\n    oi.unit_price,\n    oi.quantity * oi.unit_price AS line_total\nFROM orders o\nINNER JOIN customers c ON o.customer_id = c.customer_id\nINNER JOIN order_items oi ON o.order_id = oi.order_id\nINNER JOIN products p ON oi.product_id = p.product_id\nORDER BY o.order_id, p.product_name;\n```\n\nContinued in practice..."
    },
    {
      "title": "Python for Data Science",
      "description": "Pandas, NumPy, and data manipulation",
      "duration": "2 weeks",
      "order": 3,
      "content": "# Python for Data Science\n\n## Introduction\n\nPython has become the primary language for data science due to its simplicity and powerful libraries. This module covers the essential Python tools for data manipulation and analysis.\n\n*Content covered in AI Engineer Pathway Module 2 - Python for AI*\n\n## Additional Data Science Libraries\n\n### Pandas Advanced Techniques\n\n**Method Chaining**\n```python\nimport pandas as pd\n\nresult = (\n    df\n    .query('sales > 1000')\n    .groupby('category')\n    .agg({'sales': 'sum', 'quantity': 'mean'})\n    .sort_values('sales', ascending=False)\n    .head(10)\n)\n```\n\n**Window Functions**\n```python\n# Running total\ndf['cumulative_sales'] = df.groupby('product')['sales'].cumsum()\n\n# Moving average\ndf['moving_avg'] = df['sales'].rolling(window=7).mean()\n\n# Ranking\ndf['rank'] = df.groupby('category')['sales'].rank(ascending=False)\n```\n\n---\n\n**Next Module**: Data Visualization - creating compelling visual stories with your data!"
    },
    {
      "title": "Data Visualization",
      "description": "Tableau, Power BI, and storytelling with data",
      "duration": "2 weeks",
      "order": 4,
      "content": "# Data Visualization\n\n## The Power of Visual Communication\n\nData visualization transforms complex data into visual stories that drive understanding and action.\n\n*Content coming soon...*"
    },
    {
      "title": "Analytics Capstone",
      "description": "Complete data analysis project from scratch",
      "duration": "3 weeks",
      "order": 5,
      "content": "# Analytics Capstone Project\n\n## Building Your Data Analytics Portfolio\n\nApply everything you've learned in a comprehensive, end-to-end data analytics project.\n\n*Content coming soon...*"
    }
  ]
}

