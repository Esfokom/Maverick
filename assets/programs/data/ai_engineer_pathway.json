{
  "id": "1",
  "name": "AI Engineer Pathway",
  "shortDescription": "Master artificial intelligence and machine learning fundamentals",
  "fullDescription": "Embark on a comprehensive journey into the world of AI and machine learning. This program covers everything from fundamental concepts to advanced deep learning techniques, preparing you for a career as an AI engineer.",
  "duration": "12 weeks",
  "difficulty": "intermediate",
  "thumbnailPath": "assets/programs/ai.webp",
  "author": "Dr. Sarah Mitchell",
  "authorBio": "Ph.D. in Computer Science with 10+ years of experience in AI research and industry applications. Former lead AI scientist at major tech companies.",
  "isComingSoon": false,
  "modules": [
    {
      "title": "Introduction to AI & ML",
      "description": "Fundamental concepts, terminology, and applications",
      "duration": "2 weeks",
      "order": 1,
      "content": "# Introduction to AI & ML\n\n## Welcome to the World of Artificial Intelligence\n\nArtificial Intelligence (AI) and Machine Learning (ML) are transforming every industry, from healthcare to finance, transportation to entertainment. This module will introduce you to the fundamental concepts that power modern AI systems.\n\n## What is Artificial Intelligence?\n\nArtificial Intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn. These systems can:\n\n- **Perceive** their environment through sensors\n- **Reason** about the information they receive\n- **Learn** from experience and data\n- **Act** to achieve specific goals\n\n## Key Concepts\n\n### 1. Machine Learning\n\nMachine Learning is a subset of AI that focuses on enabling computers to learn from data without being explicitly programmed. Instead of writing specific rules, we provide examples and let the algorithm discover patterns.\n\n**Example**: Email spam filters learn to identify spam by analyzing thousands of emails, rather than following a fixed set of rules.\n\n### 2. Types of Machine Learning\n\n#### Supervised Learning\n- Learn from labeled data\n- Examples: Image classification, price prediction\n- The algorithm learns a mapping from inputs to outputs\n\n#### Unsupervised Learning\n- Find patterns in unlabeled data\n- Examples: Customer segmentation, anomaly detection\n- The algorithm discovers hidden structures\n\n#### Reinforcement Learning\n- Learn through trial and error\n- Examples: Game playing, robotics\n- The algorithm learns by receiving rewards or penalties\n\n## Real-World Applications\n\n### Healthcare\n- Disease diagnosis from medical images\n- Drug discovery and development\n- Personalized treatment recommendations\n\n### Finance\n- Fraud detection\n- Algorithmic trading\n- Credit risk assessment\n\n### Transportation\n- Self-driving cars\n- Route optimization\n- Traffic prediction\n\n### Entertainment\n- Recommendation systems (Netflix, Spotify)\n- Content generation\n- Gaming AI\n\n## The AI Development Process\n\n1. **Problem Definition**: Clearly define what you want to achieve\n2. **Data Collection**: Gather relevant, high-quality data\n3. **Data Preparation**: Clean and preprocess the data\n4. **Model Selection**: Choose appropriate algorithms\n5. **Training**: Feed data to the model to learn patterns\n6. **Evaluation**: Test the model's performance\n7. **Deployment**: Put the model into production\n8. **Monitoring**: Continuously track and improve performance\n\n## Key Terminology\n\n- **Dataset**: A collection of data used for training and testing\n- **Features**: Input variables used to make predictions\n- **Labels**: The output or target variable we want to predict\n- **Model**: The mathematical representation learned from data\n- **Training**: The process of teaching the model using data\n- **Testing**: Evaluating the model on unseen data\n- **Overfitting**: When a model learns the training data too well and performs poorly on new data\n- **Underfitting**: When a model is too simple to capture patterns in the data\n\n## Ethics in AI\n\nAs AI systems become more powerful and widespread, it's crucial to consider:\n\n- **Bias**: Ensuring AI systems are fair and don't discriminate\n- **Privacy**: Protecting personal data used in AI systems\n- **Transparency**: Making AI decisions explainable\n- **Accountability**: Determining responsibility for AI actions\n- **Safety**: Ensuring AI systems are reliable and secure\n\n## What You'll Learn Next\n\nIn the upcoming modules, you'll dive deeper into:\n\n- Python programming for AI\n- Mathematical foundations (linear algebra, calculus, probability)\n- Popular ML algorithms and when to use them\n- Deep learning and neural networks\n- Building and deploying real-world AI applications\n\n## Exercise: Reflection\n\nBefore moving on, take a moment to think about:\n\n1. What AI applications do you use in your daily life?\n2. What problem would you like to solve with AI?\n3. What excites you most about learning AI and ML?\n\nWrite down your thoughts and refer back to them as you progress through the program.\n\n---\n\n**Next Module**: Python for AI - where you'll learn the essential programming skills needed for AI development."
    },
    {
      "title": "Python for AI",
      "description": "NumPy, Pandas, and essential ML libraries",
      "duration": "2 weeks",
      "order": 2,
      "content": "# Python for AI\n\n## Why Python for AI?\n\nPython has become the de facto language for AI and Machine Learning due to:\n\n- **Simplicity**: Easy to learn and read\n- **Rich Ecosystem**: Extensive libraries for AI/ML\n- **Community**: Large, active community and resources\n- **Industry Adoption**: Used by major tech companies\n\n## Essential Python Libraries for AI\n\n### 1. NumPy: Numerical Computing\n\nNumPy is the foundation for scientific computing in Python.\n\n```python\nimport numpy as np\n\n# Creating arrays\narray_1d = np.array([1, 2, 3, 4, 5])\narray_2d = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Array operations\narray_squared = array_1d ** 2\narray_sum = np.sum(array_1d)\narray_mean = np.mean(array_1d)\n\n# Matrix operations\nmatrix_a = np.array([[1, 2], [3, 4]])\nmatrix_b = np.array([[5, 6], [7, 8]])\nmatrix_product = np.dot(matrix_a, matrix_b)\n```\n\n**Key Features**:\n- Fast operations on arrays\n- Mathematical functions\n- Linear algebra operations\n- Random number generation\n\n### 2. Pandas: Data Manipulation\n\nPandas provides powerful data structures for data analysis.\n\n```python\nimport pandas as pd\n\n# Creating a DataFrame\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'age': [25, 30, 35],\n    'score': [85, 90, 95]\n}\ndf = pd.DataFrame(data)\n\n# Reading data\ndf_csv = pd.read_csv('data.csv')\n\n# Data exploration\nprint(df.head())\nprint(df.describe())\nprint(df.info())\n\n# Data manipulation\ndf_filtered = df[df['age'] > 25]\ndf_sorted = df.sort_values('score', ascending=False)\ndf_grouped = df.groupby('age').mean()\n```\n\n**Key Operations**:\n- Loading and saving data (CSV, Excel, SQL)\n- Data cleaning and preprocessing\n- Filtering and selecting data\n- Grouping and aggregating\n- Handling missing values\n\n### 3. Matplotlib: Data Visualization\n\n```python\nimport matplotlib.pyplot as plt\n\n# Line plot\nplt.plot([1, 2, 3, 4], [1, 4, 9, 16])\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Simple Line Plot')\nplt.show()\n\n# Scatter plot\nplt.scatter(df['age'], df['score'])\nplt.xlabel('Age')\nplt.ylabel('Score')\nplt.title('Age vs Score')\nplt.show()\n\n# Histogram\nplt.hist(df['score'], bins=5)\nplt.xlabel('Score')\nplt.ylabel('Frequency')\nplt.title('Score Distribution')\nplt.show()\n```\n\n### 4. Scikit-learn: Machine Learning\n\nA comprehensive library for traditional ML algorithms.\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Prepare data\nX = df[['age']]\ny = df['score']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {mse}')\n```\n\n## Data Preprocessing\n\n### Handling Missing Data\n\n```python\n# Check for missing values\nprint(df.isnull().sum())\n\n# Fill missing values\ndf['age'].fillna(df['age'].mean(), inplace=True)\n\n# Drop rows with missing values\ndf.dropna(inplace=True)\n```\n\n### Feature Scaling\n\n```python\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Standardization (mean=0, std=1)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Normalization (0 to 1)\nscaler = MinMaxScaler()\nX_normalized = scaler.fit_transform(X)\n```\n\n### Encoding Categorical Variables\n\n```python\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# Label Encoding\nle = LabelEncoder()\ndf['category_encoded'] = le.fit_transform(df['category'])\n\n# One-Hot Encoding\ndf_encoded = pd.get_dummies(df, columns=['category'])\n```\n\n## Practical Exercise: Data Analysis Pipeline\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# 1. Load data\ndf = pd.read_csv('dataset.csv')\n\n# 2. Explore data\nprint(df.head())\nprint(df.describe())\nprint(df.isnull().sum())\n\n# 3. Preprocess data\ndf.dropna(inplace=True)\nX = df.drop('target', axis=1)\ny = df['target']\n\n# 4. Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 5. Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 6. Train model\nmodel = LogisticRegression()\nmodel.fit(X_train_scaled, y_train)\n\n# 7. Evaluate\ny_pred = model.predict(X_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\n\n# 8. Visualize results\ncm = confusion_matrix(y_test, y_pred)\nplt.imshow(cm, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.show()\n```\n\n## Best Practices\n\n1. **Always explore your data first**: Use `.head()`, `.describe()`, `.info()`\n2. **Handle missing values appropriately**: Don't just drop all missing data\n3. **Scale your features**: Most ML algorithms work better with scaled data\n4. **Split your data**: Always keep a test set separate\n5. **Vectorize operations**: Use NumPy/Pandas operations instead of loops\n6. **Document your code**: Add comments explaining your preprocessing steps\n\n## Resources for Further Learning\n\n- NumPy Documentation: https://numpy.org/doc/\n- Pandas Documentation: https://pandas.pydata.org/docs/\n- Scikit-learn Tutorials: https://scikit-learn.org/stable/tutorial/\n- Kaggle Learn: https://www.kaggle.com/learn\n\n---\n\n**Next Module**: Machine Learning Algorithms - where you'll learn the core algorithms that power AI systems."
    },
    {
      "title": "Machine Learning Algorithms",
      "description": "Supervised and unsupervised learning techniques",
      "duration": "3 weeks",
      "order": 3,
      "content": "# Machine Learning Algorithms\n\n## Overview\n\nThis module covers the fundamental machine learning algorithms that form the foundation of modern AI systems. You'll learn when to use each algorithm and how to implement them effectively.\n\n## Supervised Learning Algorithms\n\n### 1. Linear Regression\n\n**Use Case**: Predicting continuous values (house prices, stock prices, temperature)\n\n**How it works**: Finds the best-fitting line through your data points.\n\n```python\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Sample data\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([2, 4, 6, 8, 10])\n\n# Create and train model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Make prediction\nprediction = model.predict([[6]])\nprint(f'Prediction for X=6: {prediction[0]}')\n\n# Model parameters\nprint(f'Slope: {model.coef_[0]}')\nprint(f'Intercept: {model.intercept_}')\n```\n\n**Key Points**:\n- Simple and interpretable\n- Assumes linear relationship\n- Sensitive to outliers\n- Good baseline model\n\n### 2. Logistic Regression\n\n**Use Case**: Binary classification (spam/not spam, disease/no disease)\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# Binary classification\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\ny_prob = model.predict_proba(X_test)\n\n# Evaluation\nprint(classification_report(y_test, y_pred))\n```\n\n**Key Points**:\n- Despite the name, used for classification\n- Outputs probabilities\n- Works well for linearly separable data\n\n### 3. Decision Trees\n\n**Use Case**: Classification and regression with non-linear relationships\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\n# Create model\nmodel = DecisionTreeClassifier(max_depth=3, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Visualize tree\nplt.figure(figsize=(12, 8))\nplot_tree(model, filled=True, feature_names=feature_names)\nplt.show()\n\n# Feature importance\nimportances = model.feature_importances_\nfor feature, importance in zip(feature_names, importances):\n    print(f'{feature}: {importance:.4f}')\n```\n\n**Advantages**:\n- Easy to interpret\n- Handles non-linear relationships\n- No need for feature scaling\n- Works with mixed data types\n\n**Disadvantages**:\n- Prone to overfitting\n- Can be unstable\n\n### 4. Random Forest\n\n**Use Case**: Robust classification and regression\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    random_state=42\n)\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\n\n# Feature importance\nimportances = pd.DataFrame({\n    'feature': feature_names,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n```\n\n**Key Points**:\n- Ensemble of decision trees\n- More robust than single tree\n- Less prone to overfitting\n- Excellent general-purpose algorithm\n\n### 5. Support Vector Machines (SVM)\n\n**Use Case**: Classification with complex decision boundaries\n\n```python\nfrom sklearn.svm import SVC\n\n# Linear kernel\nmodel_linear = SVC(kernel='linear')\nmodel_linear.fit(X_train, y_train)\n\n# RBF kernel for non-linear data\nmodel_rbf = SVC(kernel='rbf', gamma='auto')\nmodel_rbf.fit(X_train, y_train)\n\n# Polynomial kernel\nmodel_poly = SVC(kernel='poly', degree=3)\nmodel_poly.fit(X_train, y_train)\n```\n\n**Key Points**:\n- Effective in high-dimensional spaces\n- Works well with clear margin of separation\n- Not suitable for large datasets\n- Requires feature scaling\n\n### 6. K-Nearest Neighbors (KNN)\n\n**Use Case**: Classification and regression based on similarity\n\n```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n```\n\n**How it works**:\n1. Find K nearest neighbors to a new point\n2. Take majority vote (classification) or average (regression)\n\n**Key Points**:\n- Simple and intuitive\n- No training phase\n- Slow prediction for large datasets\n- Sensitive to feature scaling\n\n## Unsupervised Learning Algorithms\n\n### 1. K-Means Clustering\n\n**Use Case**: Customer segmentation, image compression\n\n```python\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Find optimal number of clusters (Elbow method)\ninertias = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(range(1, 11), inertias)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method')\nplt.show()\n\n# Apply K-Means\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(X)\n\n# Visualize clusters\nplt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')\nplt.scatter(\n    kmeans.cluster_centers_[:, 0],\n    kmeans.cluster_centers_[:, 1],\n    marker='X', s=200, c='red'\n)\nplt.show()\n```\n\n### 2. Principal Component Analysis (PCA)\n\n**Use Case**: Dimensionality reduction, data visualization\n\n```python\nfrom sklearn.decomposition import PCA\n\n# Reduce to 2 dimensions\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\n\n# Explained variance\nprint(f'Explained variance: {pca.explained_variance_ratio_}')\n\n# Visualize\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1])\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.show()\n```\n\n## Model Evaluation\n\n### Classification Metrics\n\n```python\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix,\n    roc_auc_score,\n    roc_curve\n)\n\n# Basic metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:')\nprint(cm)\n\n# ROC Curve\ny_prob = model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nauc = roc_auc_score(y_test, y_prob)\n\nplt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()\n```\n\n### Regression Metrics\n\n```python\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    r2_score\n)\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'MSE: {mse:.4f}')\nprint(f'RMSE: {rmse:.4f}')\nprint(f'MAE: {mae:.4f}')\nprint(f'R² Score: {r2:.4f}')\n```\n\n## Cross-Validation\n\n```python\nfrom sklearn.model_selection import cross_val_score\n\n# K-Fold Cross-Validation\nscores = cross_val_score(model, X, y, cv=5)\nprint(f'Cross-validation scores: {scores}')\nprint(f'Mean accuracy: {scores.mean():.4f}')\nprint(f'Standard deviation: {scores.std():.4f}')\n```\n\n## Hyperparameter Tuning\n\n```python\nfrom sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Grid search\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy'\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(f'Best parameters: {grid_search.best_params_}')\nprint(f'Best score: {grid_search.best_score_:.4f}')\n\n# Use best model\nbest_model = grid_search.best_estimator_\n```\n\n## Algorithm Selection Guide\n\n| Problem Type | Algorithm Suggestions |\n|--------------|----------------------|\n| Linear relationships | Linear/Logistic Regression |\n| Non-linear, interpretable | Decision Trees |\n| High accuracy needed | Random Forest, Gradient Boosting |\n| Large dataset | SGD Classifier, Naive Bayes |\n| Text classification | Naive Bayes, SVM |\n| Image classification | CNN (Deep Learning) |\n| Clustering | K-Means, DBSCAN |\n| Dimensionality reduction | PCA, t-SNE |\n\n## Practical Project: Build a Complete ML Pipeline\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n# Train\npipeline.fit(X_train, y_train)\n\n# Predict\ny_pred = pipeline.predict(X_test)\n\n# Evaluate\naccuracy = pipeline.score(X_test, y_test)\nprint(f'Pipeline Accuracy: {accuracy:.4f}')\n```\n\n---\n\n**Next Module**: Deep Learning & Neural Networks - where you'll learn about the cutting-edge techniques powering modern AI."
    },
    {
      "title": "Deep Learning & Neural Networks",
      "description": "CNNs, RNNs, and modern architectures",
      "duration": "3 weeks",
      "order": 4,
      "content": "# Deep Learning & Neural Networks\n\n## Introduction to Neural Networks\n\nDeep Learning is a subset of machine learning based on artificial neural networks. These networks are inspired by the human brain and can learn complex patterns from large amounts of data.\n\n## Why Deep Learning?\n\n- **Automatic Feature Learning**: No need for manual feature engineering\n- **Handles Complex Data**: Images, text, audio, video\n- **Scales with Data**: Performance improves with more data\n- **State-of-the-art Results**: Best performance in many domains\n\n## Neural Network Basics\n\n### The Perceptron\n\nThe simplest neural network unit:\n\n```python\nimport numpy as np\n\nclass Perceptron:\n    def __init__(self, input_size, learning_rate=0.01):\n        self.weights = np.random.randn(input_size)\n        self.bias = 0\n        self.learning_rate = learning_rate\n    \n    def predict(self, x):\n        return 1 if np.dot(x, self.weights) + self.bias > 0 else 0\n    \n    def train(self, X, y, epochs=100):\n        for _ in range(epochs):\n            for xi, yi in zip(X, y):\n                prediction = self.predict(xi)\n                error = yi - prediction\n                self.weights += self.learning_rate * error * xi\n                self.bias += self.learning_rate * error\n```\n\n### Activation Functions\n\nActivation functions introduce non-linearity:\n\n```python\n# Sigmoid\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# ReLU (Rectified Linear Unit)\ndef relu(x):\n    return np.maximum(0, x)\n\n# Tanh\ndef tanh(x):\n    return np.tanh(x)\n\n# Softmax (for multi-class classification)\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x / exp_x.sum()\n```\n\n## Building Neural Networks with TensorFlow/Keras\n\n### Simple Neural Network\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Create model\nmodel = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=(784,)),\n    layers.Dropout(0.2),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(10, activation='softmax')\n])\n\n# Compile model\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=10,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1\n)\n\n# Evaluate\ntest_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {test_accuracy:.4f}')\n```\n\n### Convolutional Neural Networks (CNNs)\n\n**Use Case**: Image classification, object detection, image generation\n\n```python\n# CNN for image classification\nmodel = keras.Sequential([\n    # Convolutional layers\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    \n    layers.Conv2D(64, (3, 3), activation='relu'),\n    \n    # Dense layers\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Data augmentation\ndata_augmentation = keras.Sequential([\n    layers.RandomFlip('horizontal'),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n])\n\n# Train with augmented data\nhistory = model.fit(\n    data_augmentation(X_train),\n    y_train,\n    epochs=20,\n    validation_data=(X_val, y_val)\n)\n```\n\n**CNN Components**:\n- **Convolution**: Detect features (edges, textures, patterns)\n- **Pooling**: Reduce spatial dimensions\n- **Fully Connected**: Final classification\n\n### Recurrent Neural Networks (RNNs)\n\n**Use Case**: Sequential data, time series, text\n\n```python\n# LSTM for text classification\nmodel = keras.Sequential([\n    layers.Embedding(vocab_size, 128, input_length=max_length),\n    layers.LSTM(64, return_sequences=True),\n    layers.LSTM(32),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    epochs=10,\n    batch_size=64,\n    validation_split=0.2\n)\n```\n\n**RNN Variants**:\n- **Simple RNN**: Basic sequential processing\n- **LSTM**: Handles long-term dependencies\n- **GRU**: Simpler alternative to LSTM\n- **Bidirectional**: Processes sequences in both directions\n\n## Transfer Learning\n\nUse pre-trained models for better performance:\n\n```python\n# Load pre-trained model\nbase_model = keras.applications.ResNet50(\n    weights='imagenet',\n    include_top=False,\n    input_shape=(224, 224, 3)\n)\n\n# Freeze base model\nbase_model.trainable = False\n\n# Add custom layers\nmodel = keras.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train only custom layers\nhistory = model.fit(X_train, y_train, epochs=10)\n\n# Fine-tune: Unfreeze and train with low learning rate\nbase_model.trainable = True\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-5),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory_fine = model.fit(X_train, y_train, epochs=5)\n```\n\n## Popular Pre-trained Models\n\n- **ResNet**: Deep residual networks\n- **VGG**: Simple, effective architecture\n- **Inception**: Multi-scale feature extraction\n- **EfficientNet**: Optimized for efficiency\n- **BERT**: Transformer for NLP\n- **GPT**: Generative pre-training\n\n## Training Best Practices\n\n### 1. Learning Rate Scheduling\n\n```python\n# Reduce learning rate on plateau\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=3,\n    min_lr=1e-7\n)\n\n# Learning rate schedule\ndef scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n\nlr_schedule = keras.callbacks.LearningRateScheduler(scheduler)\n```\n\n### 2. Early Stopping\n\n```python\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True\n)\n```\n\n### 3. Model Checkpointing\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\n    'best_model.keras',\n    monitor='val_accuracy',\n    save_best_only=True\n)\n```\n\n### 4. Regularization\n\n```python\n# L2 Regularization\nlayers.Dense(\n    64,\n    activation='relu',\n    kernel_regularizer=keras.regularizers.l2(0.01)\n)\n\n# Dropout\nlayers.Dropout(0.5)\n\n# Batch Normalization\nlayers.BatchNormalization()\n```\n\n## Monitoring Training\n\n```python\nimport matplotlib.pyplot as plt\n\n# Plot training history\ndef plot_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    \n    # Accuracy\n    ax1.plot(history.history['accuracy'], label='Train')\n    ax1.plot(history.history['val_accuracy'], label='Validation')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend()\n    ax1.set_title('Model Accuracy')\n    \n    # Loss\n    ax2.plot(history.history['loss'], label='Train')\n    ax2.plot(history.history['val_loss'], label='Validation')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.legend()\n    ax2.set_title('Model Loss')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_history(history)\n```\n\n## Advanced Architectures\n\n### Autoencoders\n\n```python\n# Autoencoder for dimensionality reduction\nencoder = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=(784,)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(32, activation='relu')\n])\n\ndecoder = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=(32,)),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(784, activation='sigmoid')\n])\n\nautoencoder = keras.Sequential([encoder, decoder])\nautoencoder.compile(optimizer='adam', loss='mse')\n```\n\n### Generative Adversarial Networks (GANs)\n\n```python\n# Generator\ngenerator = keras.Sequential([\n    layers.Dense(256, activation='relu', input_shape=(100,)),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(784, activation='sigmoid'),\n    layers.Reshape((28, 28, 1))\n])\n\n# Discriminator\ndiscriminator = keras.Sequential([\n    layers.Flatten(input_shape=(28, 28, 1)),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n```\n\n## Deployment\n\n### Save and Load Models\n\n```python\n# Save entire model\nmodel.save('my_model.keras')\n\n# Load model\nloaded_model = keras.models.load_model('my_model.keras')\n\n# Save weights only\nmodel.save_weights('model_weights.h5')\nmodel.load_weights('model_weights.h5')\n```\n\n### Convert to TensorFlow Lite\n\n```python\n# Convert for mobile/edge devices\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save\nwith open('model.tflite', 'wb') as f:\n    f.write(tflite_model)\n```\n\n## Resources\n\n- TensorFlow Documentation: https://www.tensorflow.org/\n- Keras Documentation: https://keras.io/\n- Deep Learning Book: https://www.deeplearningbook.org/\n- Fast.ai Course: https://www.fast.ai/\n\n---\n\n**Next Module**: AI Project & Deployment - where you'll build and deploy your own AI application!"
    },
    {
      "title": "AI Project & Deployment",
      "description": "Build and deploy a real-world AI application",
      "duration": "2 weeks",
      "order": 5,
      "content": "# AI Project & Deployment\n\n## Building Your First AI Application\n\nIn this final module, you'll learn how to take your AI models from development to production, building a complete end-to-end application.\n\n## Project Planning\n\n### 1. Define the Problem\n- What problem are you solving?\n- Who are your users?\n- What does success look like?\n\n### 2. Gather Requirements\n- Data requirements\n- Performance requirements\n- Infrastructure constraints\n- Budget and timeline\n\n### 3. Design the System\n- Architecture diagram\n- Data pipeline\n- Model serving strategy\n- Monitoring plan\n\n## Example Project: Image Classification API\n\nWe'll build a complete image classification service:\n\n### Step 1: Prepare the Model\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Train your model\nmodel = keras.Sequential([\n    keras.applications.MobileNetV2(\n        include_top=False,\n        input_shape=(224, 224, 3)\n    ),\n    keras.layers.GlobalAveragePooling2D(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(num_classes, activation='softmax')\n])\n\n# Compile and train\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.fit(train_data, epochs=10, validation_data=val_data)\n\n# Save model\nmodel.save('image_classifier.keras')\n```\n\n### Step 2: Create a REST API with FastAPI\n\n```python\nfrom fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import JSONResponse\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image\nimport io\n\n# Initialize FastAPI app\napp = FastAPI(title=\"Image Classification API\")\n\n# Load model\nmodel = tf.keras.models.load_model('image_classifier.keras')\n\n# Class labels\nclass_labels = [\n    'cat', 'dog', 'bird', 'fish', 'horse'\n]\n\ndef preprocess_image(image_bytes):\n    \"\"\"Preprocess image for model input.\"\"\"\n    image = Image.open(io.BytesIO(image_bytes))\n    image = image.resize((224, 224))\n    image = np.array(image) / 255.0\n    image = np.expand_dims(image, axis=0)\n    return image\n\n@app.post(\"/predict\")\nasync def predict(file: UploadFile = File(...)):\n    \"\"\"Predict image class.\"\"\"\n    try:\n        # Read and preprocess image\n        contents = await file.read()\n        image = preprocess_image(contents)\n        \n        # Make prediction\n        predictions = model.predict(image)\n        predicted_class = np.argmax(predictions[0])\n        confidence = float(predictions[0][predicted_class])\n        \n        return JSONResponse(content={\n            \"class\": class_labels[predicted_class],\n            \"confidence\": confidence,\n            \"all_predictions\": {\n                label: float(prob)\n                for label, prob in zip(class_labels, predictions[0])\n            }\n        })\n    except Exception as e:\n        return JSONResponse(\n            status_code=500,\n            content={\"error\": str(e)}\n        )\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\"status\": \"healthy\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n### Step 3: Dockerize the Application\n\n```dockerfile\n# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application files\nCOPY . .\n\n# Expose port\nEXPOSE 8000\n\n# Run the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n```txt\n# requirements.txt\nfastapi==0.104.1\nuvicorn==0.24.0\ntensorflow==2.14.0\npillow==10.1.0\nnumpy==1.24.3\n```\n\n### Step 4: Deploy to Cloud\n\n#### Deploy to Google Cloud Run\n\n```bash\n# Build and push Docker image\ngcloud builds submit --tag gcr.io/PROJECT_ID/image-classifier\n\n# Deploy to Cloud Run\ngcloud run deploy image-classifier \\\n  --image gcr.io/PROJECT_ID/image-classifier \\\n  --platform managed \\\n  --region us-central1 \\\n  --allow-unauthenticated\n```\n\n#### Deploy to AWS (using EC2 or ECS)\n\n```bash\n# Build Docker image\ndocker build -t image-classifier .\n\n# Tag for ECR\ndocker tag image-classifier:latest \\\n  AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/image-classifier:latest\n\n# Push to ECR\ndocker push AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/image-classifier:latest\n```\n\n## Model Optimization\n\n### Quantization\n\nReduce model size and improve inference speed:\n\n```python\nimport tensorflow as tf\n\n# Post-training quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\n# Save optimized model\nwith open('model_quantized.tflite', 'wb') as f:\n    f.write(tflite_model)\n```\n\n### Model Pruning\n\n```python\nimport tensorflow_model_optimization as tfmot\n\n# Define pruning schedule\npruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n    initial_sparsity=0.0,\n    final_sparsity=0.5,\n    begin_step=0,\n    end_step=1000\n)\n\n# Apply pruning\nmodel_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(\n    model,\n    pruning_schedule=pruning_schedule\n)\n\n# Train pruned model\nmodel_for_pruning.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_for_pruning.fit(train_data, epochs=10)\n```\n\n## Monitoring and Logging\n\n### Application Logging\n\n```python\nimport logging\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n@app.post(\"/predict\")\nasync def predict(file: UploadFile = File(...)):\n    start_time = datetime.now()\n    \n    try:\n        logger.info(f\"Received prediction request for {file.filename}\")\n        \n        # Process and predict\n        result = process_and_predict(file)\n        \n        # Log success\n        duration = (datetime.now() - start_time).total_seconds()\n        logger.info(\n            f\"Prediction successful. \"\n            f\"Class: {result['class']}, \"\n            f\"Confidence: {result['confidence']:.2f}, \"\n            f\"Duration: {duration:.2f}s\"\n        )\n        \n        return result\n    except Exception as e:\n        logger.error(f\"Prediction failed: {str(e)}\")\n        raise\n```\n\n### Performance Monitoring\n\n```python\nfrom prometheus_client import Counter, Histogram, generate_latest\nimport time\n\n# Define metrics\nrequest_count = Counter(\n    'prediction_requests_total',\n    'Total prediction requests'\n)\n\nrequest_duration = Histogram(\n    'prediction_duration_seconds',\n    'Prediction request duration'\n)\n\n@app.post(\"/predict\")\nasync def predict(file: UploadFile = File(...)):\n    request_count.inc()\n    \n    with request_duration.time():\n        result = process_and_predict(file)\n    \n    return result\n\n@app.get(\"/metrics\")\nasync def metrics():\n    return Response(\n        generate_latest(),\n        media_type=\"text/plain\"\n    )\n```\n\n## Testing\n\n### Unit Tests\n\n```python\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom main import app\n\nclient = TestClient(app)\n\ndef test_health_check():\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"healthy\"}\n\ndef test_prediction():\n    # Load test image\n    with open(\"test_images/cat.jpg\", \"rb\") as f:\n        response = client.post(\n            \"/predict\",\n            files={\"file\": (\"cat.jpg\", f, \"image/jpeg\")}\n        )\n    \n    assert response.status_code == 200\n    data = response.json()\n    assert \"class\" in data\n    assert \"confidence\" in data\n    assert 0 <= data[\"confidence\"] <= 1\n```\n\n### Load Testing\n\n```python\nfrom locust import HttpUser, task, between\n\nclass ImageClassifierUser(HttpUser):\n    wait_time = between(1, 3)\n    \n    @task\n    def predict_image(self):\n        with open(\"test_images/sample.jpg\", \"rb\") as f:\n            self.client.post(\n                \"/predict\",\n                files={\"file\": (\"sample.jpg\", f, \"image/jpeg\")}\n            )\n```\n\n## CI/CD Pipeline\n\n### GitHub Actions Workflow\n\n```yaml\n# .github/workflows/deploy.yml\nname: Deploy AI Model\n\non:\n  push:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      \n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.9'\n      \n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install pytest\n      \n      - name: Run tests\n        run: pytest\n  \n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      \n      - name: Build and push Docker image\n        run: |\n          docker build -t myapp .\n          docker push myregistry/myapp:latest\n      \n      - name: Deploy to cloud\n        run: |\n          # Cloud deployment commands\n```\n\n## Best Practices\n\n### 1. Version Control\n- Track model versions\n- Version your data\n- Use semantic versioning\n\n### 2. Model Governance\n- Document model assumptions\n- Track model performance over time\n- Monitor for data drift\n- Maintain model cards\n\n### 3. Security\n- Validate input data\n- Use authentication/authorization\n- Encrypt sensitive data\n- Regular security audits\n\n### 4. Scalability\n- Use model caching\n- Implement load balancing\n- Consider batch processing\n- Use async processing for heavy tasks\n\n## Congratulations!\n\nYou've completed the AI Engineer Pathway! You now have the skills to:\n\n- Understand AI and ML fundamentals\n- Build and train neural networks\n- Deploy AI models to production\n- Monitor and maintain AI systems\n\n## Next Steps\n\n1. Build your own AI project\n2. Contribute to open-source AI projects\n3. Stay updated with latest research (arXiv, papers with code)\n4. Join AI communities (Kaggle, Hugging Face)\n5. Consider specializing (Computer Vision, NLP, Reinforcement Learning)\n\n## Additional Resources\n\n- MLOps: https://ml-ops.org/\n- Model Deployment: https://www.tensorflow.org/tfx\n- Cloud AI Services: AWS SageMaker, Google AI Platform, Azure ML\n- Monitoring: MLflow, Weights & Biases, Neptune.ai\n\n---\n\n**Keep learning and building amazing AI applications!**"
    }
  ]
}

