{
  "id": "2",
  "name": "Cloud Architect Program",
  "shortDescription": "Design and implement scalable cloud infrastructure solutions",
  "fullDescription": "Learn to design, build, and manage enterprise-grade cloud infrastructure. This program covers major cloud platforms (AWS, Azure, GCP) and best practices for cloud architecture, security, and cost optimization.",
  "duration": "10 weeks",
  "difficulty": "advanced",
  "thumbnailPath": "assets/programs/cloud.webp",
  "author": "James Chen",
  "authorBio": "AWS & Azure certified architect with 15 years of experience in cloud infrastructure. Previously led cloud migration projects for Fortune 500 companies.",
  "isComingSoon": false,
  "modules": [
    {
      "title": "Cloud Computing Fundamentals",
      "description": "Core concepts, service models, and deployment strategies",
      "duration": "2 weeks",
      "order": 1,
      "content": "# Cloud Computing Fundamentals\n\n## Welcome to Cloud Computing\n\nCloud computing has revolutionized how businesses build, deploy, and scale applications. This module introduces you to the core concepts and principles that underpin modern cloud architecture.\n\n## What is Cloud Computing?\n\nCloud computing is the delivery of computing services—including servers, storage, databases, networking, software, analytics, and intelligence—over the internet (\"the cloud\") to offer faster innovation, flexible resources, and economies of scale.\n\n### Key Characteristics\n\n1. **On-Demand Self-Service**\n   - Users can provision resources without human interaction\n   - Instant access to computing capabilities\n\n2. **Broad Network Access**\n   - Services available over the network\n   - Accessible through standard mechanisms\n   - Support for heterogeneous platforms\n\n3. **Resource Pooling**\n   - Multi-tenant model\n   - Resources dynamically assigned and reassigned\n   - Location independence\n\n4. **Rapid Elasticity**\n   - Scale up or down automatically\n   - Appear unlimited to consumers\n   - Pay for what you use\n\n5. **Measured Service**\n   - Usage monitoring and reporting\n   - Transparent billing\n   - Optimize resource use\n\n## Cloud Service Models\n\n### Infrastructure as a Service (IaaS)\n\n**What it is**: Virtual computing resources over the internet\n\n**You manage**: Applications, data, runtime, middleware, OS\n**Provider manages**: Virtualization, servers, storage, networking\n\n**Examples**:\n- Amazon EC2 (Elastic Compute Cloud)\n- Azure Virtual Machines\n- Google Compute Engine\n\n**Use Cases**:\n- Migrate existing applications\n- Development and test environments\n- High-performance computing\n- Big data analysis\n\n**Example: Launching an EC2 Instance**\n\n```bash\n# Using AWS CLI\naws ec2 run-instances \\\n  --image-id ami-0abcdef1234567890 \\\n  --instance-type t2.micro \\\n  --key-name my-key-pair \\\n  --security-group-ids sg-0123456789abcdef0 \\\n  --subnet-id subnet-0123456789abcdef0 \\\n  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=MyWebServer}]'\n```\n\n### Platform as a Service (PaaS)\n\n**What it is**: Platform for developing, running, and managing applications\n\n**You manage**: Applications and data\n**Provider manages**: Runtime, middleware, OS, virtualization, servers, storage, networking\n\n**Examples**:\n- AWS Elastic Beanstalk\n- Azure App Service\n- Google App Engine\n- Heroku\n\n**Use Cases**:\n- Rapid application development\n- API development and management\n- Business analytics/intelligence\n- Database management\n\n### Software as a Service (SaaS)\n\n**What it is**: Complete software solutions delivered over the internet\n\n**You manage**: Configuration and usage\n**Provider manages**: Everything else\n\n**Examples**:\n- Microsoft 365\n- Google Workspace\n- Salesforce\n- Dropbox\n\n**Use Cases**:\n- Email and collaboration\n- CRM\n- ERP\n- Any end-user application\n\n## Cloud Deployment Models\n\n### 1. Public Cloud\n\n**Characteristics**:\n- Owned and operated by third-party provider\n- Resources shared among multiple organizations\n- Accessed via internet\n\n**Advantages**:\n- No capital expenditure\n- High scalability\n- Pay-as-you-go pricing\n- No maintenance overhead\n\n**Disadvantages**:\n- Less control\n- Security concerns for sensitive data\n- Potential compliance issues\n\n**Providers**: AWS, Azure, GCP, Oracle Cloud\n\n### 2. Private Cloud\n\n**Characteristics**:\n- Dedicated to single organization\n- Can be hosted on-premises or by third party\n- Exclusive use of resources\n\n**Advantages**:\n- Greater control\n- Enhanced security and privacy\n- Customizable\n- Compliance-friendly\n\n**Disadvantages**:\n- Higher costs\n- Limited scalability\n- Maintenance responsibility\n\n**Technologies**: OpenStack, VMware vCloud, Microsoft Azure Stack\n\n### 3. Hybrid Cloud\n\n**Characteristics**:\n- Combination of public and private clouds\n- Data and applications can move between environments\n- Unified management\n\n**Advantages**:\n- Flexibility\n- Optimized costs\n- Enhanced security for sensitive data\n- Business continuity\n\n**Disadvantages**:\n- Complex architecture\n- Integration challenges\n- Requires expertise\n\n**Use Case Example**:\n```\nPrivate Cloud: Sensitive customer data, core business applications\nPublic Cloud: Web servers, development environments, burst capacity\n```\n\n### 4. Multi-Cloud\n\n**Characteristics**:\n- Use of multiple cloud providers\n- Best-of-breed approach\n- Avoid vendor lock-in\n\n**Strategy Example**:\n- AWS: Primary compute and storage\n- GCP: Machine learning and analytics\n- Azure: Enterprise applications and Office 365 integration\n\n## Major Cloud Providers Comparison\n\n| Feature | AWS | Azure | GCP |\n|---------|-----|-------|-----|\n| Market Share | ~32% | ~23% | ~10% |\n| Strengths | Mature, comprehensive | Enterprise integration | Data analytics, ML |\n| Compute | EC2 | Virtual Machines | Compute Engine |\n| Storage | S3 | Blob Storage | Cloud Storage |\n| Databases | RDS, DynamoDB | SQL Database, Cosmos DB | Cloud SQL, Firestore |\n| Serverless | Lambda | Functions | Cloud Functions |\n| Container | ECS, EKS | AKS | GKE |\n| Best For | Startups, general purpose | Enterprises, .NET apps | Data science, Kubernetes |\n\n## Cloud Architecture Principles\n\n### 1. Design for Failure\n\n**Principle**: Assume everything fails, and design backwards\n\n**Implementation**:\n- Use multiple availability zones\n- Implement health checks\n- Design for graceful degradation\n- Have backup and disaster recovery plans\n\n**Example**:\n```python\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndef get_data_with_retry(key, max_retries=3):\n    s3 = boto3.client('s3')\n    \n    for attempt in range(max_retries):\n        try:\n            response = s3.get_object(Bucket='my-bucket', Key=key)\n            return response['Body'].read()\n        except ClientError as e:\n            if attempt == max_retries - 1:\n                raise\n            print(f\"Attempt {attempt + 1} failed, retrying...\")\n            time.sleep(2 ** attempt)  # Exponential backoff\n```\n\n### 2. Decouple Your Components\n\n**Principle**: Reduce dependencies between components\n\n**Benefits**:\n- Independent scaling\n- Easier maintenance\n- Improved fault isolation\n- Flexibility in technology choices\n\n**Patterns**:\n- Message queues (SQS, Pub/Sub)\n- Event-driven architecture\n- Microservices\n- API gateways\n\n### 3. Implement Elasticity\n\n**Principle**: Scale resources up and down based on demand\n\n**Auto Scaling Example (AWS)**:\n```yaml\nResources:\n  AutoScalingGroup:\n    Type: AWS::AutoScaling::AutoScalingGroup\n    Properties:\n      MinSize: 2\n      MaxSize: 10\n      DesiredCapacity: 2\n      TargetGroupARNs:\n        - !Ref TargetGroup\n      LaunchTemplate:\n        LaunchTemplateId: !Ref LaunchTemplate\n        Version: !GetAtt LaunchTemplate.LatestVersionNumber\n  \n  ScalingPolicy:\n    Type: AWS::AutoScaling::ScalingPolicy\n    Properties:\n      AutoScalingGroupName: !Ref AutoScalingGroup\n      PolicyType: TargetTrackingScaling\n      TargetTrackingConfiguration:\n        PredefinedMetricSpecification:\n          PredefinedMetricType: ASGAverageCPUUtilization\n        TargetValue: 70.0\n```\n\n### 4. Secure at Every Layer\n\n**Principle**: Defense in depth\n\n**Layers**:\n1. Network security (VPC, subnets, security groups)\n2. Identity and access management (IAM)\n3. Data encryption (in transit and at rest)\n4. Application security (WAF, authentication)\n5. Monitoring and logging (CloudWatch, CloudTrail)\n\n**Security Best Practices**:\n```bash\n# Example: IAM policy following least privilege\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::my-bucket\",\n        \"arn:aws:s3:::my-bucket/*\"\n      ],\n      \"Condition\": {\n        \"IpAddress\": {\n          \"aws:SourceIp\": \"203.0.113.0/24\"\n        }\n      }\n    }\n  ]\n}\n```\n\n### 5. Think Parallel\n\n**Principle**: Leverage parallelization for better performance\n\n**Approaches**:\n- Multi-threaded applications\n- Distributed processing (MapReduce, Spark)\n- Parallel data loading\n- Concurrent API calls\n\n### 6. Leverage Different Storage Options\n\n| Type | Use Case | Examples |\n|------|----------|----------|\n| Object Storage | Unstructured data, backups, static websites | S3, Blob Storage |\n| Block Storage | Databases, enterprise apps | EBS, Azure Disks |\n| File Storage | Shared file systems | EFS, Azure Files |\n| Database | Structured data | RDS, DynamoDB |\n| Cache | Frequently accessed data | ElastiCache, Redis |\n| Archive | Long-term storage | Glacier, Archive Storage |\n\n## Cloud Economics\n\n### Pricing Models\n\n1. **On-Demand**: Pay for what you use, no commitments\n2. **Reserved Instances**: Commit to 1-3 years for discounts (up to 75%)\n3. **Spot Instances**: Bid on unused capacity (up to 90% off)\n4. **Savings Plans**: Flexible pricing for consistent usage\n\n### Cost Optimization Strategies\n\n```python\n# Example: Right-sizing analysis\nimport boto3\nfrom datetime import datetime, timedelta\n\ndef analyze_instance_utilization():\n    cloudwatch = boto3.client('cloudwatch')\n    ec2 = boto3.client('ec2')\n    \n    instances = ec2.describe_instances()['Reservations']\n    \n    for reservation in instances:\n        for instance in reservation['Instances']:\n            instance_id = instance['InstanceId']\n            \n            # Get CPU utilization\n            cpu_stats = cloudwatch.get_metric_statistics(\n                Namespace='AWS/EC2',\n                MetricName='CPUUtilization',\n                Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}],\n                StartTime=datetime.now() - timedelta(days=7),\n                EndTime=datetime.now(),\n                Period=3600,\n                Statistics=['Average']\n            )\n            \n            avg_cpu = sum(d['Average'] for d in cpu_stats['Datapoints']) / len(cpu_stats['Datapoints'])\n            \n            if avg_cpu < 20:\n                print(f\"Instance {instance_id} is underutilized (CPU: {avg_cpu:.2f}%)\")\n                print(f\"Consider downsizing from {instance['InstanceType']}\")\n```\n\n## Cloud Migration Strategies (6 R's)\n\n1. **Rehost** (Lift and Shift): Move as-is to cloud\n2. **Replatform** (Lift, Tinker, and Shift): Minor optimizations\n3. **Repurchase**: Move to SaaS\n4. **Refactor/Re-architect**: Redesign for cloud-native\n5. **Retire**: Decommission unnecessary applications\n6. **Retain**: Keep on-premises (for now)\n\n## Best Practices Summary\n\n✅ **Design for failure and build resilient systems**  \n✅ **Automate everything possible**  \n✅ **Use managed services when appropriate**  \n✅ **Implement comprehensive monitoring and logging**  \n✅ **Follow the principle of least privilege**  \n✅ **Optimize costs continuously**  \n✅ **Document your architecture**  \n✅ **Plan for disaster recovery**  \n\n## Hands-On Exercise\n\n**Task**: Design a three-tier web application architecture\n\n**Requirements**:\n1. High availability (99.9% uptime)\n2. Auto-scaling based on traffic\n3. Secure (data encryption, network isolation)\n4. Cost-optimized\n\n**Components to consider**:\n- Load balancer\n- Web servers\n- Application servers\n- Database\n- Caching layer\n- Content delivery\n- Backup and disaster recovery\n\n## Resources\n\n- AWS Well-Architected Framework: https://aws.amazon.com/architecture/well-architected/\n- Azure Architecture Center: https://docs.microsoft.com/en-us/azure/architecture/\n- Google Cloud Architecture Framework: https://cloud.google.com/architecture/framework\n- Cloud Computing Patterns: https://www.cloudcomputingpatterns.org/\n\n---\n\n**Next Module**: AWS Deep Dive - where you'll master Amazon Web Services!"
    },
    {
      "title": "AWS Deep Dive",
      "description": "EC2, S3, Lambda, and core AWS services",
      "duration": "2 weeks",
      "order": 2,
      "content": "# AWS Deep Dive\n\n## Introduction to Amazon Web Services\n\nAWS is the world's most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally.\n\n## Core AWS Services\n\n### 1. Amazon EC2 (Elastic Compute Cloud)\n\n**What it is**: Scalable virtual servers in the cloud\n\n#### Instance Types\n\n| Family | Use Case | Examples |\n|--------|----------|----------|\n| General Purpose | Balanced compute, memory, networking | t3, m5 |\n| Compute Optimized | High-performance processors | c5, c6g |\n| Memory Optimized | Large datasets in memory | r5, x1 |\n| Storage Optimized | High sequential read/write | i3, d2 |\n| Accelerated Computing | GPU, FPGA workloads | p4, g4 |\n\n#### Launching an EC2 Instance\n\n```python\nimport boto3\n\nec2 = boto3.resource('ec2')\n\n# Create instance\ninstances = ec2.create_instances(\n    ImageId='ami-0abcdef1234567890',\n    InstanceType='t3.micro',\n    MinCount=1,\n    MaxCount=1,\n    KeyName='my-key-pair',\n    SecurityGroupIds=['sg-0123456789abcdef0'],\n    SubnetId='subnet-0123456789abcdef0',\n    TagSpecifications=[\n        {\n            'ResourceType': 'instance',\n            'Tags': [\n                {'Key': 'Name', 'Value': 'MyWebServer'},\n                {'Key': 'Environment', 'Value': 'Production'}\n            ]\n        }\n    ],\n    UserData='''\n    #!/bin/bash\n    yum update -y\n    yum install -y httpd\n    systemctl start httpd\n    systemctl enable httpd\n    echo \"<h1>Hello from AWS</h1>\" > /var/www/html/index.html\n    '''\n)\n\nprint(f\"Launched instance: {instances[0].id}\")\n```\n\n#### Auto Scaling\n\n```python\nimport boto3\n\nautoscaling = boto3.client('autoscaling')\n\n# Create launch template\nec2 = boto3.client('ec2')\nlaunch_template = ec2.create_launch_template(\n    LaunchTemplateName='my-template',\n    LaunchTemplateData={\n        'ImageId': 'ami-0abcdef1234567890',\n        'InstanceType': 't3.micro',\n        'SecurityGroupIds': ['sg-0123456789abcdef0'],\n        'UserData': base64.b64encode(user_data_script.encode()).decode()\n    }\n)\n\n# Create Auto Scaling Group\nautoscaling.create_auto_scaling_group(\n    AutoScalingGroupName='my-asg',\n    LaunchTemplate={\n        'LaunchTemplateId': launch_template['LaunchTemplate']['LaunchTemplateId'],\n        'Version': '$Latest'\n    },\n    MinSize=2,\n    MaxSize=10,\n    DesiredCapacity=2,\n    VPCZoneIdentifier='subnet-1,subnet-2',\n    HealthCheckType='ELB',\n    HealthCheckGracePeriod=300,\n    Tags=[\n        {\n            'Key': 'Name',\n            'Value': 'AutoScaled-Instance',\n            'PropagateAtLaunch': True\n        }\n    ]\n)\n\n# Create scaling policy\nautoscaling.put_scaling_policy(\n    AutoScalingGroupName='my-asg',\n    PolicyName='scale-on-cpu',\n    PolicyType='TargetTrackingScaling',\n    TargetTrackingConfiguration={\n        'PredefinedMetricSpecification': {\n            'PredefinedMetricType': 'ASGAverageCPUUtilization'\n        },\n        'TargetValue': 70.0\n    }\n)\n```\n\n### 2. Amazon S3 (Simple Storage Service)\n\n**What it is**: Object storage service offering industry-leading scalability, availability, security, and performance\n\n#### S3 Storage Classes\n\n| Class | Use Case | Retrieval Time | Cost |\n|-------|----------|----------------|------|\n| S3 Standard | Frequently accessed | Milliseconds | $$$ |\n| S3 Intelligent-Tiering | Unknown/changing access | Milliseconds | $$ |\n| S3 Standard-IA | Infrequently accessed | Milliseconds | $$ |\n| S3 One Zone-IA | Infrequent, recreatable | Milliseconds | $ |\n| S3 Glacier Instant | Archive, instant access | Milliseconds | $ |\n| S3 Glacier Flexible | Archive, occasional access | Minutes-hours | $ |\n| S3 Glacier Deep Archive | Long-term archive | Hours | $ |\n\n#### Working with S3\n\n```python\nimport boto3\nimport json\n\ns3 = boto3.client('s3')\n\n# Create bucket\nbucket_name = 'my-unique-bucket-name'\ns3.create_bucket(\n    Bucket=bucket_name,\n    CreateBucketConfiguration={'LocationConstraint': 'us-west-2'}\n)\n\n# Enable versioning\ns3.put_bucket_versioning(\n    Bucket=bucket_name,\n    VersioningConfiguration={'Status': 'Enabled'}\n)\n\n# Enable encryption\ns3.put_bucket_encryption(\n    Bucket=bucket_name,\n    ServerSideEncryptionConfiguration={\n        'Rules': [{\n            'ApplyServerSideEncryptionByDefault': {\n                'SSEAlgorithm': 'AES256'\n            }\n        }]\n    }\n)\n\n# Set lifecycle policy\nlifecycle_policy = {\n    'Rules': [\n        {\n            'Id': 'Move to Glacier',\n            'Status': 'Enabled',\n            'Transitions': [\n                {\n                    'Days': 30,\n                    'StorageClass': 'STANDARD_IA'\n                },\n                {\n                    'Days': 90,\n                    'StorageClass': 'GLACIER'\n                }\n            ],\n            'Expiration': {'Days': 365}\n        }\n    ]\n}\n\ns3.put_bucket_lifecycle_configuration(\n    Bucket=bucket_name,\n    LifecycleConfiguration=lifecycle_policy\n)\n\n# Upload file\nwith open('myfile.txt', 'rb') as data:\n    s3.upload_fileobj(\n        data,\n        bucket_name,\n        'path/to/myfile.txt',\n        ExtraArgs={\n            'Metadata': {'uploaded-by': 'python-script'},\n            'ContentType': 'text/plain'\n        }\n    )\n\n# Generate presigned URL (temporary access)\nurl = s3.generate_presigned_url(\n    'get_object',\n    Params={'Bucket': bucket_name, 'Key': 'path/to/myfile.txt'},\n    ExpiresIn=3600  # 1 hour\n)\n\nprint(f\"Temporary URL: {url}\")\n\n# List objects\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix='path/')\nfor obj in response.get('Contents', []):\n    print(f\"Object: {obj['Key']}, Size: {obj['Size']} bytes\")\n```\n\n### 3. AWS Lambda\n\n**What it is**: Serverless compute service that runs code in response to events\n\n**Benefits**:\n- No server management\n- Automatic scaling\n- Pay only for compute time\n- Built-in fault tolerance\n\n#### Creating a Lambda Function\n\n```python\n# lambda_function.py\nimport json\nimport boto3\nfrom datetime import datetime\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Process S3 upload events and log to DynamoDB\n    \"\"\"\n    dynamodb = boto3.resource('dynamodb')\n    table = dynamodb.Table('FileUploads')\n    \n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = record['s3']['object']['key']\n        size = record['s3']['object']['size']\n        \n        # Store metadata\n        table.put_item(\n            Item={\n                'file_id': key,\n                'bucket': bucket,\n                'size': size,\n                'upload_time': datetime.now().isoformat(),\n                'processed': False\n            }\n        )\n    \n    return {\n        'statusCode': 200,\n        'body': json.dumps('Successfully processed uploads')\n    }\n```\n\n#### Deploying Lambda with boto3\n\n```python\nimport boto3\nimport zipfile\nimport io\n\nlambda_client = boto3.client('lambda')\niam = boto3.client('iam')\n\n# Create IAM role for Lambda\nrole_policy = {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n\nrole = iam.create_role(\n    RoleName='lambda-execution-role',\n    AssumeRolePolicyDocument=json.dumps(role_policy)\n)\n\n# Attach policies\niam.attach_role_policy(\n    RoleName='lambda-execution-role',\n    PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n)\n\n# Create deployment package\nzip_buffer = io.BytesIO()\nwith zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n    zip_file.write('lambda_function.py')\n\n# Create Lambda function\nresponse = lambda_client.create_function(\n    FunctionName='process-uploads',\n    Runtime='python3.9',\n    Role=role['Role']['Arn'],\n    Handler='lambda_function.lambda_handler',\n    Code={'ZipFile': zip_buffer.getvalue()},\n    Timeout=30,\n    MemorySize=256,\n    Environment={\n        'Variables': {\n            'TABLE_NAME': 'FileUploads'\n        }\n    }\n)\n\nprint(f\"Created function: {response['FunctionArn']}\")\n```\n\n### 4. Amazon RDS (Relational Database Service)\n\n**Supported Engines**: MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora\n\n```python\nimport boto3\n\nrds = boto3.client('rds')\n\n# Create DB instance\nresponse = rds.create_db_instance(\n    DBInstanceIdentifier='mydb',\n    DBInstanceClass='db.t3.micro',\n    Engine='postgres',\n    EngineVersion='13.7',\n    MasterUsername='admin',\n    MasterUserPassword='SecurePassword123!',\n    AllocatedStorage=20,\n    StorageType='gp3',\n    StorageEncrypted=True,\n    BackupRetentionPeriod=7,\n    MultiAZ=True,\n    PubliclyAccessible=False,\n    VpcSecurityGroupIds=['sg-0123456789abcdef0'],\n    DBSubnetGroupName='my-db-subnet-group',\n    Tags=[\n        {'Key': 'Environment', 'Value': 'Production'},\n        {'Key': 'Application', 'Value': 'WebApp'}\n    ]\n)\n\nprint(f\"Creating database: {response['DBInstance']['DBInstanceIdentifier']}\")\n\n# Create read replica\nrds.create_db_instance_read_replica(\n    DBInstanceIdentifier='mydb-replica',\n    SourceDBInstanceIdentifier='mydb',\n    DBInstanceClass='db.t3.micro',\n    PubliclyAccessible=False\n)\n\n# Create snapshot\nrds.create_db_snapshot(\n    DBSnapshotIdentifier='mydb-snapshot-2024-01',\n    DBInstanceIdentifier='mydb'\n)\n```\n\n### 5. Amazon VPC (Virtual Private Cloud)\n\n**What it is**: Isolated virtual network for your AWS resources\n\n```python\nimport boto3\n\nec2 = boto3.client('ec2')\n\n# Create VPC\nvpc = ec2.create_vpc(CidrBlock='10.0.0.0/16')\nvpc_id = vpc['Vpc']['VpcId']\n\nec2.modify_vpc_attribute(VpcId=vpc_id, EnableDnsHostnames={'Value': True})\nec2.modify_vpc_attribute(VpcId=vpc_id, EnableDnsSupport={'Value': True})\n\n# Create subnets\npublic_subnet = ec2.create_subnet(\n    VpcId=vpc_id,\n    CidrBlock='10.0.1.0/24',\n    AvailabilityZone='us-east-1a'\n)\n\nprivate_subnet = ec2.create_subnet(\n    VpcId=vpc_id,\n    CidrBlock='10.0.2.0/24',\n    AvailabilityZone='us-east-1a'\n)\n\n# Create and attach Internet Gateway\nigw = ec2.create_internet_gateway()\nec2.attach_internet_gateway(\n    InternetGatewayId=igw['InternetGateway']['InternetGatewayId'],\n    VpcId=vpc_id\n)\n\n# Create route table for public subnet\nroute_table = ec2.create_route_table(VpcId=vpc_id)\nec2.create_route(\n    RouteTableId=route_table['RouteTable']['RouteTableId'],\n    DestinationCidrBlock='0.0.0.0/0',\n    GatewayId=igw['InternetGateway']['InternetGatewayId']\n)\n\nec2.associate_route_table(\n    RouteTableId=route_table['RouteTable']['RouteTableId'],\n    SubnetId=public_subnet['Subnet']['SubnetId']\n)\n\n# Create NAT Gateway for private subnet\nallocation = ec2.allocate_address(Domain='vpc')\nnat_gateway = ec2.create_nat_gateway(\n    SubnetId=public_subnet['Subnet']['SubnetId'],\n    AllocationId=allocation['AllocationId']\n)\n\n# Create security group\nsecurity_group = ec2.create_security_group(\n    GroupName='web-server-sg',\n    Description='Security group for web servers',\n    VpcId=vpc_id\n)\n\nec2.authorize_security_group_ingress(\n    GroupId=security_group['GroupId'],\n    IpPermissions=[\n        {\n            'IpProtocol': 'tcp',\n            'FromPort': 80,\n            'ToPort': 80,\n            'IpRanges': [{'CidrIp': '0.0.0.0/0'}]\n        },\n        {\n            'IpProtocol': 'tcp',\n            'FromPort': 443,\n            'ToPort': 443,\n            'IpRanges': [{'CidrIp': '0.0.0.0/0'}]\n        }\n    ]\n)\n```\n\nContinued in next section..."
    },
    {
      "title": "Azure & GCP Essentials",
      "description": "Multi-cloud strategies and platform comparison",
      "duration": "2 weeks",
      "order": 3,
      "content": "# Azure & GCP Essentials\n\n## Multi-Cloud Strategy\n\nThis module covers Microsoft Azure and Google Cloud Platform, enabling you to work across multiple cloud providers and choose the best services for your needs.\n\n*Content coming soon...*"
    },
    {
      "title": "Cloud Security & Compliance",
      "description": "IAM, encryption, and regulatory compliance",
      "duration": "2 weeks",
      "order": 4,
      "content": "# Cloud Security & Compliance\n\n## Securing Cloud Infrastructure\n\nLearn best practices for securing cloud resources, managing identities, and meeting compliance requirements.\n\n*Content coming soon...*"
    },
    {
      "title": "Architecture Capstone Project",
      "description": "Design a complete cloud solution for a real scenario",
      "duration": "2 weeks",
      "order": 5,
      "content": "# Architecture Capstone Project\n\n## Building Your Cloud Solution\n\nApply everything you've learned to design and implement a complete, production-ready cloud architecture.\n\n*Content coming soon...*"
    }
  ]
}

